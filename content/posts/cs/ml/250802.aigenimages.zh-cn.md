---
title: "科普：从 CLIP 到扩散模型"
date: 2025-08-02T10:00:00+08:00
summary: "主要介绍 OpenAI 的 CLIP 模型架构及其工作原理，同时探讨扩散模型（DDPM 和 DDIM）的核心思想、数学原理以及生成图像的技术演进"
draft: false

categories: ["Machine Learning"]
tags: ["CLIP", "Diffusion Model", "Image Generation"]

cover: "https://pic.aspi-rin.top/2025/08/1559f22af4f0738f273d8a90401885cc.jpg"

katex:
  enable: true
  delimiters:
    - left: $$
      right: $$
      display: true
    - left: $
      right: $
      display: false

comments: true
showLicense: true
showRelated: true

---

参考：[https://www.youtube.com/watch?v=iv-5mZ_9CPY](https://www.youtube.com/watch?v=iv-5mZ_9CPY)

## CLIP

2021 年 2 月，OpenAI 发布了名为 CLIP 的新模型架构，基于 4 亿张图像 - 标题对组成的数据集进行训练

CLIP 由两个模型组成，处理文本的 Text Encoder 和处理图像的 Image Encoder，两个模型都输出 512 维向量（Embedding），同一组数据的输出要尽可能接近

![](https://pic.aspi-rin.top/2025/08/a78d4be269b4423800d006a6cb207372.jpg)

训练一个批次的时候，为了利用批次中的所有图像之间的相似性，会计算两两向量之间的相似度

![](https://pic.aspi-rin.top/2025/08/9c3cf0bc6b5a1369418f0af3056fd73e.jpg)

对角线上的向量是匹配的图片和标题，要最大化他们的相似性

对角线之外的都是不匹配的，要最小化他们的相似性

CLIP 中的 C 表示 Contrastive，对比，因为模型学会了对比匹配和不匹配的图像 - 标题对

- 使用余弦相似度来衡量向量的相似程度

    ![](https://pic.aspi-rin.top/2025/08/924e7c475f1df1ac63b345da18f3a6c2.jpg)

CLIP 只能将文本和图片转换到向量空间，无法反过来从向量生成图片

## DDPM → DDIM

GPT-3 的论文发表几周后，伯克利大学的一个团队发表了 Denoising Diffusion Probabilistic Models (DDPM) 论文，该论文首次表明利用扩散过程可以生成非常高质量的图像

![](https://pic.aspi-rin.top/2025/08/c5acad22255d0846f1a19a61d7cd0167.jpg)

扩散模型的核心思想是在训练图像上逐步添加噪声，直到图像完全变成噪声，然后训练神经网络计算这个过程的逆

虽然是分步添加噪声和降噪，但模型每次的目标都是返回原始图像

![](https://pic.aspi-rin.top/2025/08/e4fe2e51e91afc09b5c3c4bc8ab21332.jpg)

在生成图像的阶段也要在每一步都添加随机的噪声，DDPM 论文借鉴了一些相当复杂的理论来得到这些算法

下面介绍一种数学书等价的方法来理解扩散模型，直观的了解 DDPM 算法为何有效

假设训练图像在向量空间的分布是一个螺旋

当我们在图像中添加随机噪声时，相当于随机改变每个像素的值，类似向量空间的图片对应的向量在随机游走，也相当于物理学中的布朗运动，也是扩散模型名称的由来。

![](https://pic.aspi-rin.top/2025/08/5553f5832efb3c64da9ae85c33d7b817.jpg)

向量空间中的每个点都是一张图像

![](https://pic.aspi-rin.top/2025/08/77ef2738c249a64179b3718083e3b1a3.jpg)

我们的模型输入是从数据集的不同起点看到许多不同的随机漫步，我们的模型要做的是逆转时间，给定 t 时刻的位置，计算 t-1 时刻的位置

![](https://pic.aspi-rin.top/2025/08/a7303cdd26691f27cdab9484dc17dcc6.jpg)

实际论文上是训练模型预测整个游走过程增加的总噪声

数学上可以证明，总噪声除以行走的步数等于最后一步的噪声

效果就是大大减少了训练过程的方差（都直接预测指向起点的噪声），让模型能更有效的学习

![](https://pic.aspi-rin.top/2025/08/78f5160cd68fc89c9ae45dc2a3e7dfde.jpg)

把坐标写成函数 f，并传入一个时间变量，和随机游走的步数相对应

![](https://pic.aspi-rin.top/2025/08/5329af3010d4acfe8195ee6026b07cee.jpg)

==当 t 从 1 减小到某个阶段时，反向的向量场会突然从指向数据集中心（平均）改为指向数据集本身==

回到 DDPM 论文，反向扩散的每一步添加随机噪声（模型输出的位置增加随机步长，随着步数增加而减小步长），重复多次，可以收敛到数据集本身上

![](https://pic.aspi-rin.top/2025/08/3268e3271a3808cd7b1b4029afe4626d.jpg)

![](https://pic.aspi-rin.top/2025/08/7306c1cbab5d6fa4b92bafdc95864e16.jpg)

如果去掉随机噪声，所有的点都会迅速移动到螺旋中心，然后朝着单个内侧边缘移动，会失去整个数据集的多样性，而实际上，然而，我们的图像生成似乎并没有完全进入现实图像的流形，从而导致了模糊的非现实图像

![](https://pic.aspi-rin.top/2025/08/24fd4f3731805930db1f12343dac196c.jpg)

![](https://pic.aspi-rin.top/2025/08/411cf123c6a30359928fc0d34c19ce52.jpg)

DDPM 使扩散模型成为生成图像的一种可行方法，但扩散模型并没有立刻被广泛应用

DDPM 的关键问题在于每一步都要通过一个完整的庞大的神经网络，而且要很多步

之后，谷歌和斯坦福发表的两篇论文表示，无需要增加随机噪声，也可以能得到相同的结果

- DDPM 中的每个点的运动都可以通过一个特殊的微分方程 -「随机微分方程」表示，该方程包含模型矢量场和随机运动两个分量
- 谷歌大脑团队发现 Fokker-Planck 方程，同样可以生成与随机微分方程相同的分布，且该方程没有随机分量，是「常微分方程」，从而可以直接利用模型生成的矢量场来生成图像，无需随机扰动，该方法称为 DDIM
- DDIM 不需要对模型训练进行更改，就可以用更少的步骤，以完全确定的方式生成高质量图像

![](https://pic.aspi-rin.top/2025/08/86585664d7efc17f34c1eac2c9e0af72.jpg)

DDIM 的步长更小，从而使轨迹更好的落到螺旋分布上

## 提升效果

OpenAI 在 2022 年使用图像 - 标题对来训练扩散模型，作为 CLIP 图像编码器的逆向，称为 unCLIP，商业上叫做 DALI 2

![](https://pic.aspi-rin.top/2025/08/e6eef6f64d70912023144458d4968f47.jpg)

模型可以基于文本信息进行降噪，这种技术叫「Conditioning」

![](https://pic.aspi-rin.top/2025/08/78b2d0884434ba0e7415e3da622b9a91.jpg)

有多种将文本向量输入扩散模型的方式

但，只通过文本输入「Conditioning」画出的图，并不能包含文本中的全部信息

我们还需要一个更强的方法来指导模型

假设螺旋上的不同部分表示不同类型的图像，训练扩散模型的输入也增加对应的类别，发现在运行时，拟合程度并不高，且类别会有些混淆

![](https://pic.aspi-rin.top/2025/08/8f25a1aad4223076d2a33edc26a3d9f2.jpg)

这是因为模型在同时学习拟合螺旋和拟合对应类别两个信息，而要求点匹配螺旋的力超过了向特定类别方向移动的力

其中一种提升文本与生成图像一致性的方法称为 ==Classifier-Free Guidance==，效果显著，是现在图像和视频生成模型的重要部分，工作方式是：

==最终预测 = 无提示预测 + 指导强度 ×（有提示预测 - 无提示预测）==

- $f(x, t)$：基础的无条件噪声预测（不考虑文本提示）
- $f(x, t, prompt)$：有文本提示的噪声预测
- $\alpha(f(x, t, prompt) - f(x, t))$：指导向量，表示“有提示”与“无提示”之间的差异，并乘以指导强度 $\alpha$

![](https://pic.aspi-rin.top/2025/08/1d398f05aebd14cf442af298c8f28788.jpg)

也有负向提示的做法，除了给出希望画面包含的内容（正向提示），还可以明确告诉模型不希望出现哪些内容，工作方式和 Classifier-Free Guidance 类似：

- $f(x, t, prompt)$：用正向提示预测的结果，朝着你想要的方向引导生成。
- $f(x, t, neg\ prompt)$：用负向提示预测的结果，代表你不想要的特征。

![](https://pic.aspi-rin.top/2025/08/5168d8cff50408e9cdcb46c5843120c4.jpg)
